{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "## Part 2: Train a CNN-RNN Model\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train our CNN-RNN model.  \n",
    "\n",
    "- [Step 1](#step1): Training Setup\n",
    "  - [1a](#1a): CNN-RNN architecture\n",
    "  - [1b](#1b): Hyperparameters and other variables\n",
    "  - [1c](#1c): Image transform\n",
    "  - [1d](#1d): Data loader\n",
    "  - [1e](#1e):Loss function, learnable parameters and optimizer\n",
    "\n",
    "\n",
    "- [Step 2](#step2): Train and Validate the Model\n",
    "  - [2a](#2a): Train for the first time\n",
    "  - [2b](#2b): Resume training\n",
    "  - [2c](#2c): Notes regarding model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "We will describe the model architecture and specify hyperparameters and set other options that are important to the training procedure. We will refer to [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance.\n",
    "\n",
    "<a id='1a'></a>\n",
    "### CNN-RNN architecture\n",
    "\n",
    "For the complete CNN-RNN model, see **model.py**. For the encoder model, we use a pre-trained ResNet which has been known to achieve great success in image classification. The decoder is an RNN which has an Embedding layer, a LSTM layer and a fully-connected layer. LSTM has been shown to be successful in sequence generation.\n",
    "\n",
    "<a id='1b'></a>\n",
    "### Hyperparameters and other variables\n",
    "\n",
    "In the next code cell, we will set the values for:\n",
    "\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. We will set it to `32`.\n",
    "- `vocab_threshold` - the minimum word count threshold.  A larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary. We will set it to `5` just like [this paper](https://arxiv.org/pdf/1411.4555.pdf)\n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. This will be changed to `True` once we are done setting `vocab_threshold` and generating a `vocab.pkl` file.\n",
    "- `embed_size` - the dimensionality of the image and word embeddings. We have tried `512` as done in [this paper](https://arxiv.org/pdf/1411.4555.pdf) but it took a long time to train, so I will set it to `256`.\n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder. We will use `512` based on [this paper](https://arxiv.org/pdf/1411.4555.pdf). The larger the number, the better the RNN model can memorize sequences. However, larger numbers can significantly slow down the training process.\n",
    "- `num_epochs` - the number of epochs to train the model.  We are dealing with a huge amount of data so it will take a long time to complete even 1 epoch. Therefore, we will set `num_epochs` to `1`. We will save the model AND the optimizer every 100 training steps, and to resume training from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for any changes in vocabulary.py, data_loader.py, utils.py or model.py, and re-load it automatically.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from utils import train, validate\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Set values for the training variables\n",
    "batch_size = 32         # batch size\n",
    "vocab_threshold = 5     # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256        # dimensionality of image and word embeddings\n",
    "hidden_size = 512       # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1          # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1c'></a>\n",
    "### Image transform\n",
    "\n",
    "When setting this transform, we keep two things in mind:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- since we are using a pre-trained model, we must perform the corresponding appropriate normalization.\n",
    "\n",
    "**Training set**: As seen in the following code cell, we will set the transform for training set as follows:\n",
    "\n",
    "```python\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "```\n",
    "\n",
    "According to [this page](https://pytorch.org/docs/master/torchvision/models.html), like other pre-trained models, ResNet expects input images normalized as follows: \n",
    "- The images are expected to have width and height of at least 224. The first and second transformations resize and crop the images to 224 x 224:\n",
    "```python\n",
    "transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "```\n",
    "- The images have to be converted from numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]:\n",
    "```python\n",
    "transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "```\n",
    "- Then they are normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. This is achieved using the last transformation step:\n",
    "```python\n",
    "transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "```\n",
    "\n",
    "The data augmentation step `transforms.RandomHorizontalFlip()` improves the accuracy of the image classification task as mentioned in [this paper](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf).\n",
    "\n",
    "**Validation set**: We won't use the image augmentation step, i.e. RandomHorizontalFlip(), and will use CenterCrop() instead of RandomCrop()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Define a transform to pre-process the validation images\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1d'></a>\n",
    "### Data loader\n",
    "We will build data loaders for training and validation sets, applying the above image transforms. We will then get the size of the vocabulary from the `train_loader`, and use it to initialize our `encoder` and `decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/414113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:51<00:00, 8017.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/202654 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202654/202654 [00:24<00:00, 8158.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader, applying the transforms\n",
    "train_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "val_loader = get_loader(transform=transform_val,\n",
    "                         mode='val',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "\n",
    "# The size of the vocabulary\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1e'></a>\n",
    "### Loss function, learnable parameters and optimizer\n",
    "\n",
    "**Loss function**: We will use `CrossEntropyLoss()`.\n",
    "\n",
    "**Learnable parameters**: According to [this paper](https://arxiv.org/pdf/1411.4555.pdf), the \"loss is minimized w.r.t. all the parameters of the LSTM, the top layer of the image embedder CNN and word embeddings.\" We will follow this strategy and choose the parameters accordingly. This makes sense for two reasons:\n",
    "- the EncoderCNN in this project uses ResNet which has been pre-trained on an image classification task. So we don't have to optimize the parameters of the entire network again for a similar image classification task. We only need to optimize the top layer whose outputs are fed into the DecoderRNN.\n",
    "- the DecoderRNN is not a pre-trained network, so we have to optimize all its parameters.\n",
    "\n",
    "**Optimizer**: According to [this paper](https://arxiv.org/pdf/1502.03044.pdf), Adam optimizer works best on the MS COCO Dataset. Therefore, we will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train and Validate the Model\n",
    "\n",
    "At the beginning of this notebook, we have imported the `train` fuction and the `validate` function from `utils.py`. To figure out how well our model is doing, we will print out the training loss and perplexity during training. We will try to minimize overfitting by assessing the model's performance, i.e. the Bleu-4 score, on the validation dataset. \n",
    "\n",
    "It will take a long time to train and validate the model. Therefore we will split the training procedure into two parts: first, we will train the model for the first time and save the it every 100 steps; then we will resume, as many times as we would like or until the early stopping criterion is satisfied. We will save the model and optimizer weights in the `models` subdirectory. We will do the same for the validation procedure.\n",
    "\n",
    "First, let's calculate the total number of training and validation steps per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of training and validation steps per epoch\n",
    "total_train_step = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size)\n",
    "total_val_step = math.ceil(len(val_loader.dataset.caption_lengths) / val_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_train_step)\n",
    "print (\"Number of validation steps:\", total_val_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2a'></a>\n",
    "### Train for the first time\n",
    "\n",
    "Run the below cell if training for the first time or training continously without break. To resume training, skip this cell and run the one below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/12942], Train loss: 2.3676, Train perplexity: 10.6715\n",
      "Epoch [1/1], Step [10/6333], Val loss: 1.9856, Val perplexity: 7.2833, Val Bleu-4: 0.1056Validation Bleu-4 improved from -inf to 0.0001666988058694744, saving models to                 best-encoder.pkl and best-decoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Keep track of train and validation losses and validation Bleu-4 scores by epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_bleus = []\n",
    "# Keep track of the current best validation Bleu score\n",
    "best_val_bleu = float(\"-INF\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                       vocab_size, epoch, total_train_step)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "                                  train_loader.dataset.vocab, epoch, total_val_step)\n",
    "    val_losses.append(val_loss)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "               format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "        filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                   val_bleu, val_bleus, epoch)\n",
    "    else:\n",
    "        print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "    # Save the entire model anyway, regardless of being the best model so far or not\n",
    "    filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "    print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "    if epoch > 5:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "            break\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "### Resume training\n",
    "\n",
    "Resume training if having trained and saved the model. There are two types of data loading for training depending on where we are in the process: \n",
    "1. We will load a model from the latest training step if we are in the middle of the process and have previously saved a model, e.g. train-model-14000.pkl which means model was saved for epoch 1 at training step 4000.\n",
    "2. We will load a model saved by the below validation process after completing validating one epoch. This is when we start to train the next epoch. Therefore, we need to reset `start_loss` and `start_step` to 0.0 and 1 respectively.\n",
    "\n",
    "We will modify the code cell below depending on where we are in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/12942], Loss: 4.0215, Perplexity: 55.7821\n"
     ]
    }
   ],
   "source": [
    "# Load the last checkpoints\n",
    "checkpoint = torch.load(os.path.join('./models', 'train-model-76500.pkl'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Load start_loss from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_loss = checkpoint['total_loss']\n",
    "# Reset start_loss to 0.0 if starting a new epoch; otherwise comment it out\n",
    "#start_loss = 0.0\n",
    "\n",
    "# Load epoch. Add 1 if we start a new epoch\n",
    "epoch = checkpoint['epoch']\n",
    "# Load start_step from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_step = checkpoint['train_step'] + 1\n",
    "# Reset start_step to 1 if starting a new epoch; otherwise comment it out\n",
    "#start_step = 1\n",
    "\n",
    "# Train 1 epoch at a time due to very long training time\n",
    "train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                   vocab_size, epoch, total_train_step, start_step, start_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed training an entire epoch, we will save the necessary information. We will load pre-trained weights from the last train step `train-model-{epoch}12900.pkl`, `best_val_bleu` from `best-model.pkl` and the rest from `model-{epoch}.pkl`). We will append `train_loss` to the list `train_losses`. Then we will save the information needed for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoints\n",
    "train_checkpoint = torch.load(os.path.join('./models', 'train-model-712900.pkl'))\n",
    "epoch_checkpoint = torch.load(os.path.join('./models', 'model-6.pkl'))\n",
    "best_checkpoint = torch.load(os.path.join('./models', 'best-model.pkl'))\n",
    "\n",
    "# Load the pre-trained weights and epoch from the last train step\n",
    "encoder.load_state_dict(train_checkpoint['encoder'])\n",
    "decoder.load_state_dict(train_checkpoint['decoder'])\n",
    "optimizer.load_state_dict(train_checkpoint['optimizer'])\n",
    "epoch = train_checkpoint['epoch']\n",
    "\n",
    "# Load from the previous epoch\n",
    "train_losses = epoch_checkpoint['train_losses']\n",
    "val_losses = epoch_checkpoint['val_losses']\n",
    "val_bleus = epoch_checkpoint['val_bleus']\n",
    "\n",
    "# Load from the best model\n",
    "best_val_bleu = best_checkpoint['val_bleu']\n",
    "\n",
    "train_losses.append(train_loss)\n",
    "print (train_losses, val_losses, val_bleus, best_val_bleu)\n",
    "print (\"Training completed for epoch {}, saving model to train-model-{}.pkl\".format(epoch, epoch))\n",
    "filename = os.path.join(\"./models\", \"train-model-{}.pkl\".format(epoch))\n",
    "save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "           best_val_bleu, val_bleus, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2c'></a>\n",
    "### Notes regarding model validation\n",
    "\n",
    "- Another way to validate a model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing the model's predicted captions for the validation images. Then, write up a script or use one [available online](https://github.com/tylin/coco-caption) to calculate the BLEU score of the model. \n",
    "- Other evaluation metrics (such as TEOR and Cider) are mentioned in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf). \n",
    "\n",
    "\n",
    "# Next steps\n",
    "\n",
    "A few things that we may try in the future to improve model performance:\n",
    "\n",
    "- Adjust learning rate: make it decay over time, as in [this example](https://github.com/pytorch/examples/blob/master/imagenet/main.py).\n",
    "- Perform batch normalization.\n",
    "- Run the code on a GPU to so that we can train the model more. Currently we train for only up to 1800 steps of the first epoch.\n",
    "- Update the way we save checkpoints: save and load the correct epoch #, save the best validation Bleu-4 and val_bleus.\n",
    "- Update **Resume training** to start from the correct epoch and/or training step. We also need to ensure we load the latest best validation Bleu-4. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
