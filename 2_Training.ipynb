{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "## Part 2: Train a CNN-RNN Model\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train our CNN-RNN model.  \n",
    "\n",
    "- [Step 1](#step1): Training Setup\n",
    "  - [1a](#1a): CNN-RNN architecture\n",
    "  - [1b](#1b): Hyperparameters and other variables\n",
    "  - [1c](#1c): Image transform\n",
    "  - [1d](#1d): Data loader\n",
    "  - [1e](#1e):Loss function, learnable parameters and optimizer\n",
    "\n",
    "\n",
    "- [Step 2](#step2): Train and Validate the Model\n",
    "  - [2a](#2a): Train for the first time\n",
    "  - [2b](#2b): Resume training\n",
    "  - [2c](#2c): Notes regarding model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "We will describe the model architecture and specify hyperparameters and set other options that are important to the training procedure. We will refer to [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance.\n",
    "\n",
    "<a id='1a'></a>\n",
    "### CNN-RNN architecture\n",
    "\n",
    "For the complete CNN-RNN model, see **model.py**. For the encoder model, we use a pre-trained ResNet which has been known to achieve great success in image classification. The decoder is an RNN which has an Embedding layer, a LSTM layer and a fully-connected layer. LSTM has been shown to be successful in sequence generation.\n",
    "\n",
    "<a id='1b'></a>\n",
    "### Hyperparameters and other variables\n",
    "\n",
    "In the next code cell, we will set the values for:\n",
    "\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. We will set it to `32`.\n",
    "- `vocab_threshold` - the minimum word count threshold.  A larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary. We will set it to `5` just like [this paper](https://arxiv.org/pdf/1411.4555.pdf)\n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. This will be changed to `True` once we are done setting `vocab_threshold` and generating a `vocab.pkl` file.\n",
    "- `embed_size` - the dimensionality of the image and word embeddings. We have tried `512` as done in [this paper](https://arxiv.org/pdf/1411.4555.pdf) but it took a long time to train, so I will set it to `256`.\n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder. We will use `512` based on [this paper](https://arxiv.org/pdf/1411.4555.pdf). The larger the number, the better the RNN model can memorize sequences. However, larger numbers can significantly slow down the training process.\n",
    "- `num_epochs` - the number of epochs to train the model.  We are dealing with a huge amount of data so it will take a long time to complete even 1 epoch. Therefore, we will set `num_epochs` to `1`. We will save the model AND the optimizer every 100 training steps, and to resume training from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for any changes in vocabulary.py, data_loader.py, utils.py or model.py, and re-load it automatically.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from utils import train, validate\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Set values for the training variables\n",
    "batch_size = 32         # batch size\n",
    "vocab_threshold = 5     # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256        # dimensionality of image and word embeddings\n",
    "hidden_size = 512       # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1          # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1c'></a>\n",
    "### Image transform\n",
    "\n",
    "When setting this transform, we keep two things in mind:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- since we are using a pre-trained model, we must perform the corresponding appropriate normalization.\n",
    "\n",
    "**Training set**: As seen in the following code cell, we will set the transform for training set as follows:\n",
    "\n",
    "```python\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "```\n",
    "\n",
    "According to [this page](https://pytorch.org/docs/master/torchvision/models.html), like other pre-trained models, ResNet expects input images normalized as follows: \n",
    "- The images are expected to have width and height of at least 224. The first and second transformations resize and crop the images to 224 x 224:\n",
    "```python\n",
    "transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "```\n",
    "- The images have to be converted from numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]:\n",
    "```python\n",
    "transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "```\n",
    "- Then they are normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. This is achieved using the last transformation step:\n",
    "```python\n",
    "transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "```\n",
    "\n",
    "The data augmentation step `transforms.RandomHorizontalFlip()` improves the accuracy of the image classification task as mentioned in [this paper](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf).\n",
    "\n",
    "**Validation set**: We won't use the image augmentation step, i.e. RandomHorizontalFlip(), and will use CenterCrop() instead of RandomCrop()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Define a transform to pre-process the validation images\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1d'></a>\n",
    "### Data loader\n",
    "We will build data loaders for training and validation sets, applying the above image transforms. We will then get the size of the vocabulary from the `train_loader`, and use it to initialize our `encoder` and `decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/414113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:51<00:00, 8017.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/202654 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202654/202654 [00:24<00:00, 8158.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader, applying the transforms\n",
    "train_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "val_loader = get_loader(transform=transform_val,\n",
    "                         mode='val',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "\n",
    "# The size of the vocabulary\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1e'></a>\n",
    "### Loss function, learnable parameters and optimizer\n",
    "\n",
    "**Loss function**: We will use `CrossEntropyLoss()`.\n",
    "\n",
    "**Learnable parameters**: According to [this paper](https://arxiv.org/pdf/1411.4555.pdf), the \"loss is minimized w.r.t. all the parameters of the LSTM, the top layer of the image embedder CNN and word embeddings.\" We will follow this strategy and choose the parameters accordingly. This makes sense for two reasons:\n",
    "- the EncoderCNN in this project uses ResNet which has been pre-trained on an image classification task. So we don't have to optimize the parameters of the entire network again for a similar image classification task. We only need to optimize the top layer whose outputs are fed into the DecoderRNN.\n",
    "- the DecoderRNN is not a pre-trained network, so we have to optimize all its parameters.\n",
    "\n",
    "**Optimizer**: According to [this paper](https://arxiv.org/pdf/1502.03044.pdf), Adam optimizer works best on the MS COCO Dataset. Therefore, we will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train the Model\n",
    "\n",
    "It will take a long time to train the model. Therefore we will split the training procedure into two parts: first, we will train the model for the first time and save the it every 100 steps; then we will resume, as many times as we would like. We will save the model and optimizer weights in the `models` subdirectory.\n",
    "\n",
    "To figure out how well our model is doing, we will print out the training loss and perplexity evolve during training. We will try to minimize overfitting by assessing the model's performance, i.e. the Bleu-4 score, on the validation dataset. \n",
    "\n",
    "<a id='2a'></a>\n",
    "### Train for the first time\n",
    "We will define the number of training steps per epoch. At the beginning of this notebook, we have imported the `train` fuction and the `validate` function from `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of training steps per epoch\n",
    "total_train_step = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size)\n",
    "total_val_step = math.ceil(len(val_loader.dataset.caption_lengths) / val_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/12942], Train loss: 2.3676, Train perplexity: 10.6715\n",
      "Epoch [1/1], Step [10/6333], Val loss: 1.9856, Val perplexity: 7.2833, Val Bleu-4: 0.1056Validation Bleu-4 improved from -inf to 0.0001666988058694744, saving models to                 best-encoder.pkl and best-decoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if training for the first time. To resume training, skip this cell and run the one below it\n",
    "\n",
    "# Keep track of validation Bleu-4 scores\n",
    "val_bleus = []\n",
    "# Keep track of the current best validation Bleu score and save the model \n",
    "# when an epoch's validation Bleu score is better than best_val_bleu\n",
    "best_val_bleu = float(\"-INF\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "          epoch, num_epochs, total_train_step, vocab_size)\n",
    "    val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "          epoch, num_epochs, total_val_step, train_loader.dataset.vocab)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        torch.save({'state_dict': encoder.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                   }, os.path.join('./models', 'best-encoder.pkl'))\n",
    "        torch.save({'state_dict': decoder.state_dict()\n",
    "                   }, os.path.join('./models', 'best-decoder.pkl'))\n",
    "        print (\"Validation Bleu-4 improved from {} to {}, saving models to \\\n",
    "                best-encoder.pkl and best-decoder.pkl\".format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "    if epoch > 10:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/6333], Val loss: 2.5154, Val perplexity: 12.3717, Val Bleu-4: 0.0987"
     ]
    }
   ],
   "source": [
    "# This cell tests the validate function from utils.py, and has nothing to do with the training process\n",
    "\"\"\"\n",
    "encoder_checkpoint = torch.load(os.path.join('./models', 'encoder-11800.pkl'))\n",
    "decoder_checkpoint = torch.load(os.path.join('./models', 'decoder-11800.pkl'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(encoder_checkpoint['state_dict'])\n",
    "decoder.load_state_dict(decoder_checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(encoder_checkpoint['optimizer'])\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "          epoch, num_epochs, total_val_step, train_loader.dataset.vocab)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "### Resume training\n",
    "\n",
    "Resume training if having trained and saved the model. This can be skipped by setting `resume` to `False`. We can make a copy of this cell everytime we resume training to show the complete training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/12942], Loss: 4.0215, Perplexity: 55.7821\n"
     ]
    }
   ],
   "source": [
    "resume = True\n",
    "if resume:\n",
    "    # Load data as dictionary from the last checkpoint\n",
    "    encoder_checkpoint = torch.load(os.path.join('./models', 'encoder-1100.pkl'))\n",
    "    decoder_checkpoint = torch.load(os.path.join('./models', 'decoder-1100.pkl'))\n",
    "    \n",
    "    # Load the pre-trained weights\n",
    "    encoder.load_state_dict(encoder_checkpoint['state_dict'])\n",
    "    decoder.load_state_dict(decoder_checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(encoder_checkpoint['optimizer'])\n",
    "    \n",
    "    # Keep track of validation Bleu-4 scores\n",
    "    val_bleus = []\n",
    "    # Keep track of the current best validation Bleu score and save the model \n",
    "    # when an epoch's validation Bleu score is better than best_val_bleu\n",
    "    best_val_bleu = float(\"-INF\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "              epoch, num_epochs, total_train_step, vocab_size)\n",
    "        val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "              epoch, num_epochs, total_val_step, train_loader.dataset.vocab)\n",
    "        val_bleus.append(val_bleu)\n",
    "        if val_bleu > best_val_bleu:\n",
    "            torch.save({'state_dict': encoder.state_dict(),\n",
    "                        'optimizer' : optimizer.state_dict(),\n",
    "                       }, os.path.join('./models', 'best-encoder.pkl'))\n",
    "            torch.save({'state_dict': decoder.state_dict()\n",
    "                       }, os.path.join('./models', 'best-decoder.pkl'))\n",
    "            print (\"Validation Bleu-4 improved from {} to {}, saving models to \\\n",
    "                    best-encoder.pkl and best-decoder.pkl\".format(best_val_bleu, val_bleu))\n",
    "            best_val_bleu = val_bleu\n",
    "        if epoch > 10:\n",
    "            # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "            if early_stopping(val_bleus, 3):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2c'></a>\n",
    "### Notes regarding model validation\n",
    "\n",
    "- Another way to validate a model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing the model's predicted captions for the validation images. Then, write up a script or use one [available online](https://github.com/tylin/coco-caption) to calculate the BLEU score of the model. \n",
    "- Other evaluation metrics (such as TEOR and Cider) are mentioned in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf). \n",
    "\n",
    "\n",
    "# Next steps\n",
    "\n",
    "A few things that we may try in the future to improve model performance:\n",
    "\n",
    "- Adjust learning rate: make it decay over time, as in [this example](https://github.com/pytorch/examples/blob/master/imagenet/main.py).\n",
    "- Perform batch normalization.\n",
    "- Run the code on a GPU to so that we can train the model more. Currently we train for only up to 1800 steps of the first epoch.\n",
    "- Update the way we save checkpoints: save and load the correct epoch #, save the best validation Bleu-4 and val_bleus.\n",
    "- Update **Resume training** to start from the correct epoch and/or training step. We also need to ensure we load the latest best validation Bleu-4. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
